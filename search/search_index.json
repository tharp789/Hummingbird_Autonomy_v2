{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AirStack Boilerplate","text":"<p>Welcome to the AirStack Boilerplate! This repository template serves to kickstart the development of your own robotics autonomy stack. You're encouraged to customize your version in any way to best suit your project's needs.</p> <p>This boilerplate is maintained by the AirLab at Carnegie Mellon University's Robotics Institute.</p> <p>Please head to our Getting Started page to start.</p> <p> AirStack</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at {{ email }}. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"common/","title":"Index","text":"<p>Common top-level files for sub workspaces.</p> <p><code>ros_packages/</code> contains common ROS packages that are used between different machines, such as ground control station and the robot. This folder is mounted under the docker container under <code>~/ros_ws/src/common</code></p>"},{"location":"docs/","title":"AirStack Boilerplate","text":"<p>Welcome to the AirStack Boilerplate! This repository template serves to kickstart the development of your own robotics autonomy stack. You're encouraged to customize your version in any way to best suit your project's needs.</p> <p>This boilerplate is maintained by the AirLab at Carnegie Mellon University's Robotics Institute.</p> <p>Please head to our Getting Started page to start.</p> <p> AirStack</p>"},{"location":"docs/about/","title":"About","text":"<p>This stack is built and maintained by the AirLab at Carnegie Mellon University's Robotics Institute. </p>"},{"location":"docs/about/#license","title":"License","text":"<p>Not sure yet but probably Apache 2.0 or MIT for the open source parts.</p>"},{"location":"docs/about/#faq","title":"FAQ","text":"<p>Phasellus posuere in sem ut cursus</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"docs/getting_started/","title":"Getting Started","text":"<p>By the end of this tutorial, you will have the autonomy stack running on your machine.</p>"},{"location":"docs/getting_started/#requirements","title":"Requirements","text":"<p>You need at least 25GB free to install the Docker image.</p> <p>Have an NVIDIA GPU &gt;= RTX 3070 to run Isaac Sim locally.</p>"},{"location":"docs/getting_started/#setup","title":"Setup","text":""},{"location":"docs/getting_started/#clone","title":"Clone","text":"<pre><code>git clone --recursive -j8 git@github.com:castacks/AirStack.git\n</code></pre>"},{"location":"docs/getting_started/#docker","title":"Docker","text":"<p>Follow NVIDIA's instructions for installing Docker to be compatible with NVIDIA GPUs, including adding the NVIDIA Container Toolkit. Make sure <code>docker-compose-plugin</code> is also installed with Docker.</p>"},{"location":"docs/getting_started/#configure","title":"Configure","text":"<p>Run <code>./configure.sh</code> and follow the instructions in the prompts to do an initial configuration of the repo.</p>"},{"location":"docs/getting_started/#docker-images","title":"Docker Images","text":"<p>Now you have two options on how to proceed. You can build the docker image from scratch or pull the existing image on the airlab docker registry. Building the image from scratch can be useful if you would like to add new dependencies or add new custom functionality. For most users just pulling the existing image will be more conveninent and fast since it doesn't require access to the Nvidia registry.</p> Option 1: Pull From the Airlab Registry (Preferred) To use the AirLab Docker registry do the following  <pre><code>cd AirStack/\ndocker login airlab-storage.andrew.cmu.edu:5001\n## &lt;Enter your andrew id (without @andrew.cmu.edu)&gt;\n## &lt;Enter your andrew password&gt;\n\n## Pull the images in the docker compose file\ndocker compose pull\n</code></pre>  The images will be pulled from the server automatically. This might take a while since the images are large.   Option 2: Build Docker Images From Scratch  1.  Download the Ascent Spirit SITL software package by running this script (pip3 is required):      <pre><code>cd AirStack/\nbash simulation/isaac-sim/installation/download_sitl.bash\n</code></pre>  2.  Next, gain access to NVIDIA NGC Containers by following these instructions.      Then:      <pre><code>cd AirStack/\ndocker compose build  # build the images locally\n</code></pre>  If you have permission you can push updated images to the docker server.  <pre><code>docker compose push\n</code></pre>"},{"location":"docs/getting_started/#launch","title":"Launch","text":"<pre><code>xhost +  # allow docker access to X-Server\n\n# Make sure you are in the AirStack directory.\n\n# Start docker compose services. This launches Isaac Sim and the robots.\n#  You can append `--scale robot=[NUM_ROBOTS]` for more robots, default is 1\ndocker compose up -d\n</code></pre> <p>Then open the stage from the Nucleus server: <code>airlab-storage.andrew.cmu.edu:8443/Projects/AirStack/neighborhood.scene.usd</code></p>"},{"location":"docs/getting_started/#move-robot","title":"Move Robot","text":"<p>Find the RQT GUI window. Hit <code>Takeoff</code>, then hit <code>Publish</code> in the trajectory window like in this video:</p> <p>Note you can also use the <code>ros2 topic pub</code> command to move the robot. For example, to fly to a position:</p> <pre><code># start another terminal in docker container\ndocker exec -it airstack-robot-1 bash\n# in docker\n# FLY TO POSITION. Put whatever position you want\nros2 topic pub /robot_1/interface/mavros/setpoint_position/local geometry_msgs/PoseStamped \\\n    \"{ header: { stamp: { sec: 0, nanosec: 0 }, frame_id: 'base_link' }, \\\n    pose: { position: { x: 10.0, y: 0.0, z: 20.0 }, orientation: { x: 0.0, y: 0.0, z: 0.0, w: 1.0 } } }\" -1\n</code></pre>"},{"location":"docs/getting_started/#shutdown","title":"Shutdown","text":"<p>To shutdown and remove docker containers:</p> <pre><code>docker compose down\n</code></pre>"},{"location":"docs/development/","title":"Developer Guide","text":"<p>Welcome developers! This guide documents how to extend the autonomy stack for your own needs. The stack has been designed with modularity in mind, and aims to make it straight forward to swap out any component.</p> <p>We assume you're developing first on a local machine with simulation.</p>"},{"location":"docs/development/contributing/","title":"Contributing","text":"<p>This page describes how to merge content back into main.</p>"},{"location":"docs/development/contributing/#dependencies","title":"Dependencies","text":"<p>Make sure to add your ROS2 package dependencies to your <code>package.xml</code> file. These get installed when the docker image is built.</p> <p>If you need to add a dependency that's not in the docker image, please add a section to the <code>Dockerfile</code> in the <code>docker/</code> directory.</p>"},{"location":"docs/development/contributing/#documentation","title":"Documentation","text":"<p>Please make sure to document your work. Docs are under <code>AirStack/docs/</code>. The navigation tree is under <code>AirStack/mkdocs.yml</code>.</p> <p>This documentation is built with Material MKDocs. Visit mkdocs.org and mkdocs-material to learn how to use it.</p>"},{"location":"docs/development/contributing/#commands","title":"Commands","text":"<p><pre><code>pip install mkdocs-material\nmkdocs serve\n</code></pre> Launches docs on https://localhost:8000.</p> <ul> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"docs/development/contributing/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"docs/development/contributing/#merge","title":"Merge","text":"<p>Submit a pull request.</p> <p>All tests must pass before merging.</p> <p>Regression tests are run so that we don't break anything.</p>"},{"location":"docs/development/create_new_project/","title":"Create a New Project","text":""},{"location":"docs/development/create_new_project/#option-1-generate-from-template-no-further-updates-from-upstream","title":"Option 1: Generate from Template (no further updates from upstream)","text":"<p>The AirStack repository is setup as a Template on GitHub. This makes it easy to create a new project from AirStack with the \"Use this template\" button.</p> <p></p> <p>However, generating from a template squashes the entire git history into a single starter commit in your new repository.  This prevents pulling in updates from the AirStack repository in the future.</p> <p>To be able to pull upstream changes from the AirStack repository, use option 2.</p>"},{"location":"docs/development/create_new_project/#option-2-duplicate-for-future-updates-from-upstream","title":"Option 2: Duplicate (for future updates from upstream)","text":"<p>Duplicating a repository preserves the entire history of the repository, making it easier to pull in updates from AirStack in the future. Unlike creating a fork, duplicating a repository allows your new repository to be private.</p> <p>See GitHub's instructions to duplicate a repository.</p> <p>Sample commands are provided below: <pre><code>git clone --bare https://github.com/castacks/AirStack.git my-airstack\ncd my-airstack\ngit push --mirror https://github.com/EXAMPLE-USER/my-airstack.git\n</code></pre></p>"},{"location":"docs/development/docker_usage/","title":"General Usage with Docker Compose","text":"<p>To mimic interacting with multiple real world robots, we use Docker Compose to manage Docker containers that isolate the simulation, each robot, and the ground control station.</p> <p>The details of the docker compose setup is in <code>AirStack/docker-compose.yaml</code>.</p> <p>In essence, the compose file launches:</p> <ul> <li>Isaac Sim</li> <li>ground control station</li> <li>robots</li> </ul> <p>all get created on the same default Docker bridge network. This lets them communicate with ROS2 on the same network.</p> <p>Each robot has its own ROS_DOMAIN_ID.</p>"},{"location":"docs/development/docker_usage/#pull-images","title":"Pull Images","text":"<p>To use the AirLab docker registry:</p> <pre><code>cd AirStack/\ndocker login airlab-storage.andrew.cmu.edu:5001\n## &lt;Enter your andrew id (without @andrew.cmu.edu)&gt;\n## &lt;Enter your andrew password&gt;\n\n## Pull the images in the docker compose file\ndocker compose pull\n</code></pre> <p>Catelog: AirLab Registry Images.</p> <p>Available image tags: airstack-dev, isaac-sim_ros-humble</p>"},{"location":"docs/development/docker_usage/#build-images","title":"Build Images","text":"<pre><code>docker compose build\n</code></pre>"},{"location":"docs/development/docker_usage/#start-stop-and-remove","title":"Start, Stop, and Remove","text":"<p>Start</p> <pre><code>docker compose up -d --scale robot=[NUM_ROBOTS]\n\n# see running containers\ndocker ps -a\n</code></pre> <p>Stop</p> <pre><code>docker compose stop\n</code></pre> <p>Remove</p> <pre><code>docker compose down\n</code></pre>"},{"location":"docs/development/docker_usage/#isaac-sim","title":"Isaac Sim","text":"<p>Start a bash shell in the Isaac Sim container:</p> <pre><code># if the isaac container is already running, execute a bash shell in it\ndocker exec -it isaac-sim bash\n# or if not, start a new container\ndocker compose run isaac-sim bash\n</code></pre> <p>Within the isaac-sim Docker container, the alias <code>runapp</code> launches Isaac Sim. The <code>--path</code> argument can be passed with a path to a <code>.usd</code> file to load a scene.</p> <p>It can also be run in headless mode with <code>./runheadless.native.sh</code> to stream to Omniverse Streaming Client or <code>./runheadless.webrtc.sh</code> to stream to a web browser.</p> <p>The container also has the isaacsim ROS2 package within that can be launched with <code>ros2 launch isaacsim run_isaacsim.launch.py</code>.</p>"},{"location":"docs/development/docker_usage/#robot","title":"Robot","text":"<p>Start a bash shell in a robot container, e.g. for robot_1:</p> <pre><code>docker exec -it airstack-robot-1 bash\n</code></pre> <p>The previous <code>docker compose up</code> launches robot_bringup in a tmux session. To attach to the session within the docker container, e.g. to inspect output, run <code>tmux attach</code>.</p> <p>The following commands are available within the robot container:</p> <pre><code># in robot docker\ncws  # cleans workspace\nbws  # builds workspace\nbws --packages-select [your_packages] # builds only desired packages\nsws  # sources workspace\nros2 launch robot_bringup robot.launch.xml  # top-level launch\n</code></pre> <p>These aliases are in <code>AirStack/robot/.bashrc</code>.</p> <p>Each robot has <code>ROS_DOMAIN_ID</code> set to its ID number. <code>ROBOT_NAME</code> is set to <code>robot_$ROS_DOMAIN_ID</code>.</p>"},{"location":"docs/development/docker_usage/#ground-control-station","title":"Ground Control Station","text":"<p>Currently the ground control station uses the same image as the robot container. This may change in the future.</p> <p>Start a bash shell in a robot container:</p> <pre><code>docker exec -it ground-control-station bash\n</code></pre> <p>The available aliases within the container are currently the same.</p> <p>On the GCS <code>ROS_DOMAIN_ID</code> is set to 0.</p>"},{"location":"docs/development/docker_usage/#ssh-into-robots","title":"SSH into Robots","text":"<p>The containers mimic the robots' onboard computers on the same network. Therefore we intend to interface with the robots through ssh.</p> <p>The <code>ground-control-station</code> and <code>docker-robot-*</code> containers are setup with ssh daemon, so you can ssh into the containers using the IP address.</p> <p>You can get the IP address of each container by running the following command:</p> <pre><code>docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' [CONTAINER-NAME]\n</code></pre> <p>Then ssh in, for example:</p> <pre><code>ssh root@172.18.0.6\n</code></pre> <p>The ssh password is <code>airstack</code>.</p>"},{"location":"docs/development/docker_usage/#container-details","title":"Container Details","text":"<pre><code>graph TD\n    A(Isaac Sim) &lt;-- Sensors and Actuation --&gt; B\n    A &lt;-- Sensors and Actuation --&gt; C\n    B(Robot 1) &lt;-- Global Info --&gt; D(Ground Control Station)\n    C(Robot 2) &lt;-- Global Info --&gt; D\n\n    style A fill:#76B900,stroke:#333,stroke-width:2px\n    style B fill:#fbb,stroke:#333,stroke-width:2px\n    style C fill:#fbb,stroke:#333,stroke-width:2px\n    style D fill:#fbf,stroke:#333,stroke-width:2px\n</code></pre>"},{"location":"docs/development/frame_conventions/","title":"Frame conventions","text":"<p>Isaac Sim follows the Forward-Left-Up (FLU) coordinate system. This means that the X-axis points forward, the Y-axis points left, and the Z-axis points up. This is the same convention used in the Isaac SDK and Isaac Sim's parent platform, Omniverse. Isaac Docs</p>"},{"location":"docs/development/testing/","title":"Testing","text":""},{"location":"docs/development/testing/ci_cd/","title":"CI/CD Pipeline","text":""},{"location":"docs/development/testing/integration_testing/","title":"Integration Testing","text":""},{"location":"docs/development/testing/testing_frameworks/","title":"Testing Frameworks","text":""},{"location":"docs/development/vscode/","title":"VS Code: Docker Integration and Debugger Setup","text":"<p>Start containers <pre><code># optionally pass the --scale robot=N argument to start N robots\ndc compose up -d  # --scale robot=2\n</code></pre></p> <p>Open AirStack folder</p> <pre><code>cd AirStack\ncode .\n</code></pre> <p>Install the \"Dev Containers\" extension.</p> <p>Now click the \"Remote Explorer\" icon on the left side bar, hover over a robot container, and attach to the container.</p> <p></p> <p>Install recommended extensions within the image. This installs the <code>ROS</code>, <code>C++</code>, and <code>Python</code> extensions in the container. </p>"},{"location":"docs/development/vscode/#build-ros-workspace","title":"Build ROS Workspace","text":"<p>Hit <code>Ctrl-Shift-B</code> to build the project. This is a shortcut for <code>bws --cmake-args '-DCMAKE_BUILD_TYPE=Debug'</code>, which adds debug symbols to the build.</p> <p>Build tasks are defined in <code>.vscode/tasks.json</code>.</p>"},{"location":"docs/development/vscode/#launch","title":"Launch","text":"<p>Hit <code>F5</code> to launch <code>robot.launch.xml</code>, or click the \"Run and Debug\" button on the left side of the screen and click the green play button.</p> <p>Launch tasks are defined in <code>.vscode/launch.json</code>.</p> <p></p> <p>You can now set breakpoints, view variables, step-through code, and debug as usual in VSCode.</p> <p></p> <p>Warning about file permissions</p> <p>Folders and files created within the attached docker container will be owned by root. This can cause issues when trying to edit files from the host machine, especially when using git to switch branches. If you accidentally create files as root, you can change the owner to your user with the following command: <pre><code>sudo chown -R $USER:$USER .\n</code></pre></p>"},{"location":"docs/ground_control_station/","title":"Ground Control Station","text":"<p>The Ground Control Station (GCS) is for operators to monitor and control the robots. </p>"},{"location":"docs/real_world/","title":"Real World Overview","text":"<p>Fly robots in da wild.</p> <p>That's wild.</p>"},{"location":"docs/real_world/installation/","title":"Installation on Hardware","text":""},{"location":"docs/robot/","title":"Robot","text":""},{"location":"docs/robot/#launch-structure","title":"Launch Structure","text":"<p>Each high-level module has a <code>*_bringup</code> package that contains the launch files for that module. The launch files are located in the <code>launch</code> directory of the <code>*_bringup</code> package. The launch files are named <code>*.launch.(xml/yaml/py)</code> and can be launched with <code>ros2 launch &lt;module_name&gt;_bringup &lt;module_name&gt;.launch.(xml/yaml/py)</code>.</p>"},{"location":"docs/robot/common_topics/","title":"Common topics","text":"Topic Type Description <code>/$ROBOT_NAME/odometry</code> nav_msgs/Odometry Best estimate of robot odometry <code>/$ROBOT_NAME/global_plan</code> nav_msgs/Path Current target global trajectory for the robot to follow. See global planning for more details."},{"location":"docs/robot/common_topics/#system-diagram","title":"System Diagram","text":""},{"location":"docs/robot/autonomy/","title":"Autonomy Modules","text":""},{"location":"docs/robot/autonomy/#modules","title":"Modules","text":"<ul> <li>0_interface</li> <li>1_sensors</li> <li>2_perception</li> <li>3_local</li> <li>4_global</li> <li>5_behavior</li> </ul>"},{"location":"docs/robot/autonomy/#system-diagram","title":"System Diagram","text":""},{"location":"docs/robot/autonomy/0_interface/","title":"Robot Interface","text":"<p>The interface defines the communication between the autonomy stack running on the onboard computer and the robot's control unit. For example, for drones it converts the control commands from the autonomy stack into MAVLink messages for the flight controller.</p> <p>TODO: This is not our diagram, must replace. </p> <p>The code is located under <code>AirStack/ros_ws/src/robot/autonomy/0_interface/</code>.</p>"},{"location":"docs/robot/autonomy/0_interface/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/0_interface/interface_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch interface_bringup interface.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/0_interface/#robotinterface","title":"RobotInterface","text":"<p>Package <code>robot_interface</code> is a ROS2 node that interfaces with the robot's hardware. The <code>RobotInterface</code> gets robot state and forwards it to the autonomy stack, and also translates control commands from the autonomy stack into the command for the underlying hardware. Note the base class is unimplemented. Specific implementations should extend <code>class RobotInterface</code> in <code>robot_interface.hpp</code>, for example <code>class MAVROSInterface</code>.</p>"},{"location":"docs/robot/autonomy/0_interface/#state","title":"State","text":"<p>The <code>RobotInterface</code> class broadcasts the robot's pose as a TF2 transform. It also publishes the robot's odometry as a <code>nav_msgs/Odometry</code> message to <code>$(env ROBOT_NAME)/0_interface/robot_0_interface/odometry</code>.</p>"},{"location":"docs/robot/autonomy/0_interface/#commands","title":"Commands","text":"<p>The commands are variations of the two main command modes: Attitude control and Position control. These are reflected in MAVLink and supported by both PX4 and Ardupilot.</p> <p>The RobotInterface node subscribes to:</p> <ul> <li><code>/$(env ROBOT_NAME)/interface/cmd_attitude_thrust</code> of type <code>mav_msgs/AttitudeThrust.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_rate_thrust</code> of type <code>mav_msgs/RateThrust.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_roll_pitch_yawrate_thrust</code> of type <code>mav_msgs/RollPitchYawrateThrust.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_torque_thrust</code> of type <code>mav_msgs/TorqueThrust.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_velocity</code> of type <code>geometry_msgs/TwistStamped.msg</code></li> <li><code>/$(env ROBOT_NAME)/interface/cmd_position</code> of type <code>geometry_msgs/PoseStamped.msg</code></li> </ul> <p>All messages are in the robot's body frame, except <code>velocity</code> and <code>position</code> which use the frame specified by the message header.</p>"},{"location":"docs/robot/autonomy/0_interface/#mavrosinterface","title":"MAVROSInterface","text":"<p>The available implementation in AirStack is called <code>MAVROSInterface</code> implemented in <code>mavros_interface.cpp</code>. It simply forwards the control commands to the Ascent flight controller (based on Ardupilot) using MAVROS.</p>"},{"location":"docs/robot/autonomy/0_interface/#custom-robot-interface","title":"Custom Robot Interface","text":"<p>If you're using a different robot control unit with its own custom API, then you need to create an associated RobotInterface. Implementations should do the following:</p>"},{"location":"docs/robot/autonomy/0_interface/#broadcast-state","title":"Broadcast State","text":"<p>Implementations of <code>RobotInterface</code> should obtain the robot's pose and broadcast it as a TF2 transform.</p> <p>Should look something like:</p> <pre><code>// callback function triggered by some loop\nvoid your_callback_function(){\n    // ...\n    geometry_msgs::msg::TransformStamped t;\n    // populate the transform, e.g.:\n    t.header = // some header\n    t.transform.translation.x = // some value\n    t.transform.translation.y = // some value\n    t.transform.translation.z = // some value\n    t.transform.rotation = // some quaternion\n    // Send the transformation\n    this-&gt;tf_broadcaster_-&gt;sendTransform(t);\n    // ...\n}\n</code></pre> <p>TODO: our code doesn't currently do it like this, it instead uses an external odometry_conversion node.</p>"},{"location":"docs/robot/autonomy/0_interface/#override-command-handling","title":"Override Command Handling","text":"<p>Should override all <code>virtual</code> functions in <code>robot_interface.hpp</code>:</p> <ul> <li><code>cmd_attitude_thrust_callback</code></li> <li><code>cmd_rate_thrust_callback</code></li> <li><code>cmd_roll_pitch_yawrate_thrust_callback</code></li> <li><code>cmd_torque_thrust_callback</code></li> <li><code>cmd_velocity_callback</code></li> <li><code>cmd_position_callback</code></li> <li><code>request_control</code></li> <li><code>arm</code></li> <li><code>disarm</code></li> <li><code>is_armed</code></li> <li><code>has_control</code></li> </ul>"},{"location":"docs/robot/autonomy/1_sensors/","title":"Index","text":"<p>We'll fill this with different things like the ZED-X package, LiDAR, etc</p>"},{"location":"docs/robot/autonomy/1_sensors/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/sensors/sensors_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch sensors_bringup sensors.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/2_perception/","title":"Perception","text":"<p>These modules process raw sensor data into useful information for the robot. For example: for detecting obstacles, localizing the robot, and recognizing objects.</p> <p>Perception modules typically output topics in image space or point cloud space. This information then gets aggregated into global and local world models later in the pipeline.</p> <p>Common perception modules include:</p> <ul> <li>semantic segmentation</li> <li>VIO (Visual Inertial Odometry)</li> </ul>"},{"location":"docs/robot/autonomy/2_perception/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/perception/perception_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch perception_bringup perception.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/2_perception/state_estimation/","title":"State estimation","text":"<p>Hey Yuheng your stuff goes here:)</p>"},{"location":"docs/robot/autonomy/3_local/","title":"Local Packages","text":"<p>The local module includes packages that are specific to the local autonomy of the robot. This includes local mapping, planning, and control.</p>"},{"location":"docs/robot/autonomy/3_local/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/local/local_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch local_bringup local.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/3_local/controls/","title":"Controls","text":"<p>Controls dictate the actuation of the robot. They are responsible for taking in sensor data and producing control commands. </p> <p>The controller should publish control commands directly to topics defined by the Robot Interface.</p> <p>Currently the AirStack uses a custom controller called \"Trajectory Controller\".</p>"},{"location":"docs/robot/autonomy/3_local/planning/","title":"Local Planning","text":"<p>Part of the local planner is the Waypoint Manager.</p> <p>The Waypoint Manager subscribes to the global waypoints and the drone's current position and publishes the next waypoint to the local planner.</p> <p>We plan for this baseline to be DROAN</p>"},{"location":"docs/robot/autonomy/3_local/world_model/","title":"Local World Model","text":""},{"location":"docs/robot/autonomy/4_global/","title":"Global Packages","text":"<p>The global packages include global world models and planners.</p>"},{"location":"docs/robot/autonomy/4_global/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/global/global_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch global_bringup global.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/4_global/planning/","title":"Planning","text":"<p>Global planners output a high level, coarse trajectory for the robot to follow. </p> <p>A trajectory is a spatial path plus a schedule.  This means each waypoint in the trajectory has a time associated with it, indicating when the robot should reach that waypoint. These timestamps are fed to the local planner and controller to determine velocity and acceleration.</p> <p>If a waypoint's header timestamp is empty, the local planner should assume there's no time constraint and follow the trajectory at its own pace.</p> <p>The global planner should make a trajectory that is collision-free according to the global map. However, avoiding fine obstacles is delegated to the local planner that operates at a faster rate.</p> <p>For the structure of the package, the global planner node should not include any logic to generate the path. This should be located in a seperate logic class and be seperated from ROS. This will allow more modularity in the future for testing and easy interface changes.</p> <p>We intend the global planners to be modular. AirStack implements a basic Random Walk planner as a baseline.  Feel free to implement your own through the following interfaces.</p>"},{"location":"docs/robot/autonomy/4_global/planning/#ros-interfaces","title":"ROS Interfaces","text":"<p>Global planners are meant to be modules that can be swapped out easily.  They can be thought of as different high level behaviors for the robot to follow. Consider that multiple global planners may be run in parallel, for example by some ensemble planner node that chooses the best plan for the current situation.</p> <p>As such, the global planner should be implemented as a ROS2 node that accepts runtime mission parameters in a custom <code>PlanRequest.msg</code> and  publishes a plan to its local <code>~/global_plan</code> topic.</p> <p>The best global plan should then be forwarded or remapped to <code>/$(env ROBOT_NAME)/global_plan</code> for the local planner to follow.</p> <pre><code>sequenceDiagram\n  autonumber\n  Global Manager-&gt;&gt;Global Planner: ~/plan_request (your_planner/PlanRequest.msg)\n  loop Planning\n      Global Planner--&gt;&gt;Global Manager: heartbeat feedback\n  end\n  Global Planner-&gt;&gt;Global Manager: ~/global_plan (nav_msgs/Path.msg)\n  Global Manager-&gt;&gt;Local Planner: /$ROBOT_NAME/global_plan_reference (nav_msgs/Path.msg)\n  Local Planner-&gt;&gt;Global Manager: /$ROBOT_NAME/global_plan_eta (nav_msgs/Path.msg)</code></pre>"},{"location":"docs/robot/autonomy/4_global/planning/#subscribe-plan-request","title":"Subscribe: Plan Request","text":"<p>Your custom <code>PlanRequest.msg</code> defines the parameters that your global planner needs to generate a plan.  It will be sent on the <code>~/plan_request</code> topic.</p> <p>Some common parameters may be the following: <pre><code># PlanRequest.msg\nstd_msgs/Duration timeout  # maximum time to spend planning\ngeometry_msgs/Polygon bounds # boundary that the plan must stay within\n</code></pre></p>"},{"location":"docs/robot/autonomy/4_global/planning/#publish-global-plan","title":"Publish: Global Plan","text":"<p>The global planner must publish a message of type <code>nav_msgs/Path</code> to <code>~/global_plan</code>. The message defines high level waypoints to reach by a given time.</p> <p>The <code>nav_msgs/Path</code> message type contains a <code>header</code> field and <code>poses</code> field.</p> <ul> <li>The top level header of <code>nav_msgs/Path</code> message should contain the coordinate frame of the trajectory, and its timestamp should indicate when the trajectory was published.</li> <li>Within the <code>poses</code> field, each <code>geometry_msgs/PoseStamped</code>'s header should contain a timestamp that indicates when that waypoint should be reached</li> </ul> <pre><code>nav_msgs/Path.msg\n    - std_msgs/Header header\n        - time stamp: when the trajectory was generated\n        - frame_id: the coordinate frame of the trajectory\n    - geometry_msgs/PoseStamped[] poses: the trajectory\n        - geometry_msgs/PoseStamped pose\n            - std_msgs/Header header\n                - time stamp: when the waypoint should be reached\n                - string frame_id: the coordinate frame of the waypoint\n            - geometry_msgs/Pose pose: the position and orientation of the waypoint\n</code></pre>"},{"location":"docs/robot/autonomy/4_global/planning/#publish-heartbeat","title":"Publish: Heartbeat","text":"<p>For long-running global planners, it's recommended to publish a heartbeat message to <code>~/heartbeat</code>. This way the calling node can know that the global planner is still running and hasn't crashed.</p>"},{"location":"docs/robot/autonomy/4_global/planning/#additional-subscribers","title":"Additional Subscribers","text":"<p>In general, the global planner needs to access components of the world model such as the map and drone state.</p> <p>The most common map is Occupancy Grids that is published by TODO node.</p> <p>The global planner can also access the robot's current state and expected state in the future. For example, if the global planner takes 20 seconds to plan a trajectory,  it can query where the robot expects to be in 20 seconds. This ROS2 service is available under TODO.</p> <p>The global planner can do whatever it wants internally with this information.</p>"},{"location":"docs/robot/autonomy/4_global/planning/#example-planners","title":"Example Planners","text":""},{"location":"docs/robot/autonomy/4_global/planning/#random-walk-planner","title":"Random Walk planner","text":"<p>The random walk planner replans when the robot is getting close to the goal. The random walk planner is a trivial planner that generates a plan by randomly selecting a direction to move in. The random walk planner is useful for testing the robot's ability to follow a plan.</p>"},{"location":"docs/robot/autonomy/4_global/world_model/","title":"World Model","text":"<p>Global world models are responsible for maintaining a representation of the world that is used by the global planner to generate a plan. This representation is typically a map of the environment, but can also include other information such as the location of other robots, obstacles, and goals.</p> <p>The current placeholder world model is a voxelized map representation called VDB Mapping.</p>"},{"location":"docs/robot/autonomy/5_behavior/","title":"Behavior","text":"<p>The behavior module is responsible for the high-level decision making of the robot. This includes deciding what actions to take based on the current state of the robot and the world around it. The behavior module is responsible for coordinating the actions of the local and global modules to achieve the robot's goals.</p>"},{"location":"docs/robot/autonomy/5_behavior/#launch","title":"Launch","text":"<p>Launch files are under <code>src/robot/autonomy/behavior/behavior_bringup/launch</code>.</p> <p>The main launch command is <code>ros2 launch behavior_bringup behavior.launch.xml</code>.</p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_executive/","title":"Behavior Executive","text":"<p>The behavior executive reads which actions are active from the behavior tree and implements the behavior which these actions should perform and sets the status of the actions to SUCCESS, RUNNING, or FAILURE. It also sets the status of conditions as either SUCCESS or FAILURE.</p> <p>A typical way of implementing the behavior for an action is the following in the 20 Hz timer callback:</p> <pre><code>if(action-&gt;is_active()){\n  if(action-&gt;active_has_changed()){\n    // This is only true when the when the action transitions between active/inactive\n    // so this block of code will only run once whenever the action goes from being inactive to active.\n    // You might put a service call here and then call action-&gt;set_success() or action-&gt;set_failure()\n    // based on the result returned by the service call.\n  }\n\n  // Code here will get executed each iteration.\n  // You might call action-&gt;set_running() while you are doing work here.\n}\n</code></pre>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/","title":"Behavior Trees","text":"<p>De\ufb01nes how a task in terms of conditions and actions which the user implements.</p> <p>Other types of nodes, control \ufb02ow and decorator nodes, control which conditions will be checked and which actions will be activated.</p> <p>Nodes have statuses of either SUCCESS, RUNNING or FAILURE.</p> <p></p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#why-behavior-trees","title":"Why Behavior Trees?","text":"<p>Maintainable - Easy to modify</p> <p>Scalable - Parts of sub-trees are modular and can be encapsulated</p> <p>Reusable - Sub-trees can be reused in different places</p> <p>Clear visualization and interpretation</p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#types-of-nodes","title":"Types of Nodes","text":"<ul> <li>Execution Nodes<ul> <li>Condition Nodes</li> <li>Action Nodes</li> </ul> </li> <li>Decorator Nodes<ul> <li>Not Node</li> </ul> </li> <li>Control Flow Nodes<ul> <li>Sequence Nodes</li> <li>Fallback Nodes</li> </ul> </li> </ul>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#execution-nodes-condition-nodes","title":"Execution Nodes - Condition Nodes","text":"<p>Condition nodes have a status of either SUCCESS or FAILURE</p> <p> </p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#execution-nodes-action-nodes","title":"Execution Nodes - Action Nodes","text":"<p>Action nodes can either be active or inactive</p> <p>An inactive node's status is not checked by the behavior tree, it is shown in white</p> <p>below</p> <p>An active node's status is checked, it can either be SUCCESS (green), RUNNING (blue) or FAILURE (red)</p> <p> </p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#decorator-nodes-not-nodes","title":"Decorator Nodes - Not Nodes","text":"<p>The not node must have one condition node has a child and inverts the status of the child.</p> <p>If the child's status is SUCCESS, the not node's status will be FAILURE.</p> <p>If the child's status is FAILURE, the not node's status will be SUCCESS.</p> <p></p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#control-flow-nodes-fallback-nodes","title":"Control Flow Nodes - Fallback Nodes","text":"<p>These nodes are shown with a ?</p> <p>This node returns FAILURE if and only if all of its children return FAILURE</p> <p>If one of its children return RUNNING or SUCCESS, it returns RUNNING or SUCCESS and no subsequent children's statuses are check</p> <p>Below shows a typical example, where an action will only be performed if all of the preceding conditions are false. In this case a drone will only be armed if it is not already armed, it is in o\ufb00board mode and it is stationary</p> <p></p>"},{"location":"docs/robot/autonomy/5_behavior/behavior_tree/#control-flow-nodes-sequence-nodes","title":"Control Flow Nodes - Sequence Nodes","text":"<p>These nodes are shown with a \"-&gt;\"</p> <p>This node returns SUCCESS if and only if all of its children return SUCCESS</p> <p>If one of its children return RUNNING or FAILURE, it returns RUNNING or FAILURE and no subsequent children's statuses are check</p> <p>Below shows a typical example where preceding conditions must be true in order for an action to be performed. In this case the drone will land if the IMU times out and it is in o\ufb00board mode</p> <p></p>"},{"location":"docs/robot/logging/","title":"Logging","text":""},{"location":"docs/robot/static_transforms/","title":"Index","text":""},{"location":"docs/robot/static_transforms/#frame-conventions","title":"Frame Conventions","text":"<p>Each robot has its own map frame that represents the starting position of the robot. The map frame is expected to be in ENU (East-North-Up) convention. </p> <p>The robot is in the base_link frame.</p>"},{"location":"docs/simulation/","title":"Simulation","text":"<p>We primarily support Isaac Sim. In the future we plan to support Gazebo.</p>"},{"location":"docs/simulation/docker_network/","title":"Docker network","text":""},{"location":"docs/simulation/docker_network/#overview","title":"Overview","text":"<p>The details of the docker containers setup is in the <code>docker-compose.yaml</code> file in the <code>AirStack</code> directory.</p> <p>Isaac Sim, the ground control station, and robots all get created on the same default Docker bridge network. This lets them communicate with ROS2 on the same network.</p> <p>Each robot has its own ROS_DOMAIN_ID.</p>"},{"location":"docs/simulation/docker_network/#ssh-into-robots","title":"SSH into Robots","text":"<p>The <code>ground-control-station</code> and <code>docker-robot</code> containers are setup with ssh daemon, so you can ssh into the containers using the IP address.</p> <p>You can get the IP address of each container by running the following command:</p> <pre><code>docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' [CONTAINER-NAME]\n</code></pre> <p>Then ssh in, for example:</p> <pre><code>ssh root@172.18.0.6\n</code></pre> <p>The ssh password is <code>airstack</code>.</p>"},{"location":"docs/simulation/isaac_sim/","title":"Isaac Sim","text":"<p>The primary simulator we support is NVIDIA Isaac Sim.  We chose Isaac Sim as the best balance between photorealism and physics simulation.</p>"},{"location":"docs/simulation/isaac_sim/#usd-file-naming-conventions","title":"USD File Naming Conventions","text":"<p>AirStack uses the following file naming conventions:</p> <p>Purely 3D graphics</p> <ul> <li> <p><code>*.prop.usd</code> \u27f5 simply a 3D model with materials, typically encompassing just a single object. Used for individual assets or objects (as mentioned earlier), representing reusable props.</p> </li> <li> <p><code>*.stage.usd</code> \u27f5 an environment composed of many props, but with no physics, no simulation, no robots. simply scene graphics</p> </li> </ul> <p>Simulation-ready</p> <ul> <li> <p><code>*.robot.usd</code> \u27f5 a prop representing a robot plus ROS2 topic and TF publishers, physics, etc.</p> </li> <li> <p><code>*.scene.usd</code> \u27f5 an environment PLUS physics, simulation, or robots</p> </li> </ul>"},{"location":"docs/simulation/isaac_sim/ascent_sitl_extension/","title":"AirLab AirStack Extension","text":"<p>The AirStack extension for IsaacSim does two main things. It creates an Ascent Omnigraph Node which runs the Ascent SITL and updates the position of a drone model in IsaacSim based on the SITL. It also creates a panel for listing, attaching to, and killing tmux sessions.</p>"},{"location":"docs/simulation/isaac_sim/ascent_sitl_extension/#ascent-omnigraph-node","title":"Ascent OmniGraph Node","text":"<p>The Ascent OmniGraph node takes as input a domain id, node namespace and drone prim. It runs the Ascent SITL, mavproxy, and mavros and takes care of keeping the SITL time synced with IsaacSim's time. Mavros is run using the inputted domain id and node namespace. The drone prim's position is set based off of the position of the drone in the SITL. The drone prim doesn't do collision and will pass through objects in the IsaacSim world.</p> <p>The way the SITL is synced with IsaacSim is by running the SITL in gdb with a breakpoint on the functin that advances the SITL time. Every time this function is called, our code is run by injecting a library using the LD_PRELOAD trick. Our code runs a client socket that talks to a server socket running in the AirStack IsaacSim extension which tells it how long to sleep based off the current SITL and IsaacSim time.</p> <p>The Ascent OmniGraph node is shown below:</p> <p></p>"},{"location":"docs/simulation/isaac_sim/ascent_sitl_extension/#tmux-panel","title":"TMUX Panel","text":"<p>This is a panel for listing, attaching to, and killing any running TMUX sessions. The Ascent SITL, mavproxy, and mavros are run in a TMUX sesion, so this is mainly for debugging those and probably doesn't need to be interacted with by most users. A list of TMUX sessions is displayed in the panel. It doesn't auto refresh so you have to manually click the refresh button to display any changes in the list of sessions. For each session, there is an <code>Attach</code> button and a <code>Kill</code> button. The <code>Attach</code> button will bring up an <code>xterm</code> window with the TMUX session. The <code>Kill</code> button will kill the TMUX session.</p> <p>The TMUX panel is shown below:</p> <p></p>"},{"location":"docs/simulation/isaac_sim/export_stages_from_unreal/","title":"Export Unreal Engine to Isaac Sim","text":"<p>A robot needs a scene to interact in. A scene can be created in any 3D modeling program, though we have found it easiest to export stages from Unreal Engine. This document explains how to export an Unreal Engine environment to an Isaac Sim stage, then how to convert the stage to a physics-enabled scene. </p>"},{"location":"docs/simulation/isaac_sim/export_stages_from_unreal/#exporting-unreal-engine-environments-to-isaac-sim-stages","title":"Exporting Unreal Engine Environments to Isaac Sim Stages","text":"<p>Generally, Unreal Engine environments can be found on Epic Games' Fab Marketplace. For example, the Open World Demo Collection is a free collection of outdoor environments.</p> <p>The below video explains how to export an Unreal Engine environment to an Isaac Sim stage.</p> <p>You can save this file as <code>[YOUR_ENVIRONMENT_NAME].stage.usd</code>.</p>"},{"location":"docs/simulation/isaac_sim/export_stages_from_unreal/#export-tips","title":"Export Tips","text":"<p>Complexity: Generally, it works best to export levels that are designed for UE4.27 and below. The advanced rendering features from UE5, i.e. nanite and lumen, aren't compatible with Omniverse.  If the level is optimized for the older UE4, then it's more compatible.</p> <p>Omniverse doesn't perform well with large amounts of vegetation. Anything with complex vegetation takes a long long time to load. Procedural foliage doesn't export well either. Simple geometries like buildings and rocks work better. </p> <p>That said you can still achieve photorealism by substituting complex geometries for high quality textures. Isaac seems to do fine with high quality textures.</p> <p>Optimization: After exporting, edit the file with USD Composer and run the Scene Optimizer extension for faster performance. USD Composer can be installed via Omniverse Launcher.</p> <p>Verify the Scale:  The Omniverse exporter exports in centimeters, but Isaac Sim natively works in meters. For consistency, follow these steps to change the scene units to be meters.</p> <p>To check the scale of the scene, you can add a cube in Isaac Sim and compare it to the exported scene. The cube is 1m x 1m x 1m.</p>"},{"location":"docs/simulation/isaac_sim/export_stages_from_unreal/#turn-a-stage-into-a-physics-enabled-scene","title":"Turn a Stage into a Physics-Enabled Scene","text":"<p>Adding physics to the stage is as simple as adding a <code>Physics</code> property with the \"Colliders Preset\", as described in the Isaac docs. Then save the scene as <code>[YOUR_ENVIRONMENT_NAME].scene.usd</code> to clarify that it's a physics-enabled scene.</p> <p>You're now ready to add robots to the scene on the next page.</p>"},{"location":"docs/simulation/isaac_sim/scene_setup/","title":"AirStack Scene Setup","text":""},{"location":"docs/simulation/isaac_sim/scene_setup/#creating-a-new-scene-with-robots","title":"Creating a New Scene with Robots","text":"<p>The easiest way is to reference and copy an existing scene.</p>"},{"location":"docs/simulation/isaac_sim/scene_setup/#ros-publishers-through-omnigraph","title":"ROS Publishers Through OmniGraph","text":""},{"location":"docs/simulation/isaac_sim/scene_setup/#configure-robot-name-ros_domain_id-and-topic-namespaces","title":"Configure Robot Name, ROS_DOMAIN_ID, and Topic Namespaces","text":"<p>Under the Spirit drone prim is an <code>ActionGraph</code> component, which is an Omnigraph. This component is used to configure the ROS publishers for the robot. The <code>ActionGraph</code> component has the following fields to configure:</p> <ul> <li><code>robot_name</code>: The name of the robot. This is used as the top-level namespace for ROS topics.</li> <li><code>domain_id</code>: The ROS domain ID. This is used as the <code>ROS_DOMAIN_ID</code> for DDS networking.</li> </ul> <p>The Omnigraph has subgraphs for each ROS publisher type. For example, TFs, Images, and PointClouds. The top-level <code>robot_name</code> and <code>domain_id</code> fields get fed into the subgraphs. The <code>Topic Namespaces</code> field should be set to the topic namespace in the subgraphs. This is used to namespace the ROS topics.</p> <p></p>"},{"location":"docs/simulation/isaac_sim/scene_setup/#customizing-the-omnigraph","title":"Customizing the Omnigraph","text":"<p>Common pre-built graphs for ROS may be added through the top menu bar: <code>Isaac Utils &gt; Common OmniGraphs</code>. This is helpful for creating various sensor publishers.</p> <p>We recommend organizing your work into sub-graphs. Copy your omnigraph template them into the top-level <code>Omnigraph</code> component, named \"ActionGraph\". Connect the <code>robot_name</code> and <code>domain_id</code> fields to your workflow. Then, select all the nodes in your workflow, right-click, and create a subgraph.</p>"},{"location":"ground_control_station/installation/","title":"Index","text":"<p>Scripts to install on machine, todo. Maybe something with ansible? Bash scripts could work too.</p>"},{"location":"ground_control_station/ros_ws/","title":"Index","text":"<p>Eventually we will have some ground control station that enables commanding all robots simultaneously.</p> <p>Envision a GUI like an RTS game, where you have a global 3D map you can move around, and can also see and control your \"units\" on the map. A minimap will help with understanding where you are. Envision the map being built up over time in 3D as your agents explore.</p>"},{"location":"ground_control_station/ros_ws/src/mission_manager/","title":"Mission Manager","text":"<p>This package handles the allocation of tasks for the multiple agents. It assigned agents to search or track. For search it divides the search space.</p> <pre><code>colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release\n</code></pre> <p>Debugging this node <pre><code>ros2 run --prefix 'gdb -ex run --args'  mission_manager mission_manager_node\n</code></pre></p> <pre><code>ros2 launch mission_manager mission_manager_launch.py\nros2 run rviz2 rviz2\n</code></pre>"},{"location":"ground_control_station/ros_ws/src/ros2tak_tools/","title":"TAK Tools ROS 2 Package","text":"<p><code>ros2tak_tools</code> is a ROS 2 package designed for integrating TAK (Tactical Assault Kit) functionalities within ROS 2. It includes tools for publishing and subscribing to Cursor-On-Target (CoT) events, interfacing with TAK servers, and setting up search and rescue missions.</p>"},{"location":"ground_control_station/ros_ws/src/ros2tak_tools/#features","title":"Features","text":"<ul> <li>ROS 2 to TAK Communication: Send ROS 2 messages to TAK using CoT events.</li> <li>CoT to ROS 2 Communication: Receive CoT events from TAK and publish as ROS 2 messages.</li> <li>Mission Planning: Custom tools to create and manage search missions using CoT and ROS.</li> </ul>"},{"location":"robot/installation/","title":"Index","text":"<p>Scripts to install on machine, todo. Maybe something with ansible? Bash scripts could work too.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/a_world_models/disparity_expansion/","title":"README","text":"<p>This package generates a world representation using disparity images. This enables planning in image space by applying C-space expansion in 2.5D disparity images.</p> <p>It generates a pair expansion images: one for foreground expansion and one for background expansion.</p> <p>Currently this package also has the following nodes</p> <ul> <li> <p><code>disparity_expansion</code>: Generate expanded disparity</p> </li> <li> <p><code>disparity_pcd</code>: Disparity image to point cloud.</p> </li> </ul>"},{"location":"robot/ros_ws/src/autonomy/3_local/a_world_models/disparity_expansion/#who-do-i-talk-to","title":"Who do I talk to?","text":"<ul> <li>geetesh dubey (geeteshdubey@gmail.com)</li> </ul>"},{"location":"robot/ros_ws/src/autonomy/3_local/a_world_models/disparity_expansion/#license","title":"License","text":"<p>BSD, see LICENSE</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/b_planners/takeoff_landing_planner/","title":"Index","text":""},{"location":"robot/ros_ws/src/autonomy/3_local/b_planners/takeoff_landing_planner/#takeofflandingplanner","title":"TakeoffLandingPlanner","text":"<p>Author:</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/mav_comm/","title":"mav_comm","text":"<p>This repository contains message and service definitions used for mavs. All future message definitions go in here, existing ones in other stacks should be moved here where possible.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/","title":"px4_msgs","text":"<p>ROS 2 message definitions for the PX4 Autopilot project.</p> <p>Building this package generates all the required interfaces to interface ROS 2 nodes with the PX4 internals.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#supported-versions-and-compatibility","title":"Supported versions and compatibility","text":"<p>Depending on the PX4 and ROS versions you want to use, you need to checkout the appropriate branch of this package:</p> PX4 ROS 2 Ubuntu branch v1.13 Foxy Ubuntu 20.04 release/1.13 v1.14 Foxy Ubuntu 20.04 release/1.14 v1.14 Humble Ubuntu 22.04 release/1.14 v1.14 Rolling Ubuntu 22.04 release/1.14 main Foxy Ubuntu 22.04 main main Humble Ubuntu 22.04 main main Rolling Ubuntu 22.04 main"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#messages-sync-from-px4","title":"Messages Sync from PX4","text":"<p>When PX4 message definitions in the <code>main</code> branch of PX4 Autopilot change, a CI/CD pipeline automatically copies and pushes updated ROS message definitions to this repository. This ensures that this repository <code>main</code> branch and the PX4-Autopilot <code>main</code> branch are always up to date. However, if you are using a custom PX4 version and you modified existing messages or created new one, then you have to manually synchronize them in this repository:</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#manual-message-sync","title":"Manual Message Sync","text":"<ul> <li>Checkout the correct branch associated to the PX4 version from which you detached you custom version.</li> <li>Delete all <code>*.msg</code> files in <code>msg/</code> and copy all <code>*.msg</code> files from <code>PX4-Autopilot/msg/</code> in it. Assuming that this repository and the PX4-Autopilot repository are placed in your home folder, you can run:   <pre><code>rm -f ~/px4_msgs/msg/*.msg\ncp ~/PX4-Autopilot/msg/*.msg ~/px4_msgs/msg/\n</code></pre></li> </ul>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#install-build-and-usage","title":"Install, build and usage","text":"<p>Check Using colcon to build packages to understand how this can be built inside a workspace. Check the PX4 ROS 2 User Guide section on the PX4 documentation for further details on how this integrates PX4 and how to exchange messages with the autopilot.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#bug-tracking-and-feature-requests","title":"Bug tracking and feature requests","text":"<p>Use the Issues section to create a new issue. Report your issue or feature request here.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/#questions-and-troubleshooting","title":"Questions and troubleshooting","text":"<p>Reach the PX4 development team on the PX4 Discord Server.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/CONTRIBUTING/","title":"Contributing","text":"<p>Do not commit changes directly to this repository that change the message definitions. All the message definitions are directly generated from the uORB msg definitions on the PX4 Firmware repository. Any fixes or improvements one finds suitable to apply to the message definitions should be directly done on the uORB message files. The deployment of these are taken care by a Jenkins CI/CD stage.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/px4_msgs/CONTRIBUTING/#contributing-to-the-px4-firmware-repository-or-to-this-repository-not-including-message-definitions","title":"Contributing to the PX4 Firmware repository (or to this repository, not including message definitions)","text":"<p>Follow the <code>Contributing</code> guide from the PX4 Firmware repo.</p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/","title":"Trajectory Controller","text":""},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#overview","title":"Overview","text":"<p>The trajectory controller takes in a trajectory and publishes a tracking point, which a controller should make the drone fly to, and a look ahead point, which a planner should plan from. The trajectory controller can interpret a trajectory as a standalone complete track, like a figure eight or racetrack pattern for tuning controls, or as separate segments that should be stitched together, for example trajectories output by a local planner.</p> <p>The trajectory controller tries to keep the tracking point ahead of the robot in a pure pursuit fashion. The robot's position, the red X in the figure below, is projected onto the trajectory. A sphere, shown in 2d as the cyan circle, is placed around this projected point and the intersection between the sphere and the forward point on the trajectory is used as the tracking point, the blue X. The lookahead point, the orange X, is a fixed time duration along the trajectory.</p> <p></p>"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#parameters","title":"Parameters","text":"Parameter Description <code>tf_prefix</code> The tf names published are prefixed with the string in this parameter. Tfs are published at the tracking point and lookahead point. There are stabilized versions, the same but with zero pitch and roll, of these two tfs that are also published. <code>target_frame</code> The tracking point and lookahead point are published in this frame. <code>look_ahead_time</code> How far ahead of the tracking point the lookahead point will be in seconds. <code>min_virtual_tracking_velocity</code> If the velocity on the trajectory is less than this, the tracking point will just move forward in time instead of using the sphere to keep the tracking point ahead of the drone. The units are m/s. <code>sphere_radius</code> This is the radius of the sphere used to determine the position of the tracking point. Making it larger pushes the tracking point farther ahead. <code>search_ahead_factor</code> To search for the point on the trajectory that intersects with the sphere, the algorithm checks a certain distance ahead along the trajectory. This distance is given by <code>sphere_radius * search_ahead_factor</code>. If the trajectory zigzags a lot relative to the size of the sphere, it's possible that the algorithm wouldn't iterate far enough along the trajectory to find the point where it intersects with the sphere. If a large value for this parameter is used and the trajectory loops back on itself, it is possible that this would cause the tracking point to jump ahead and skip a portion of the trajectory. In almost all cases, this parameter shouldn't need to be changed. <code>traj_vis_thickness</code> The thickness of the trajectory visualization markers. ## Services"},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#trajectory-modes","title":"Trajectory Modes","text":"<p>There are several modes that the trajectory controller can be placed in with a service call to the <code>set_trajectory_mode</code> service: See TrajectoryMode.srv.</p> Mode Description TRACK This interprets a trajectory subscribed on <code>~/trajectory_override</code> as a complete trajectory that the controller will follow. It is usually used for taking off, landing, and tuning controls on fixed trajectories like figure eights, racetracks, circles, etc... ADD_SEGMENT This interprets a trajectory subscribed on <code>~/trajectory_segment_to_add</code> as a segment of a trajectory which will get stitched onto the current trajectory at the closest point to the start of the new segment. This is usually published by a local planner. Ideally it is published at the location of the lookahead point, which is a fixed time ahead of the tracking point. This fixed time should be greater than the time it takes to plan. For example, if the lookahead point is one second ahead of the tracking point, the local planner should be always take less than one second to plan otherwise the tracking point would already be past the start of the plan. If this happens, the trajectory will fail to be stitched and will be ignored. PAUSE This causes the tracking point to stop where it is. REWIND This makes the tracking point go backwards along the trajectory. This mode is usually used to make the drone blindly backtrack along its trajectory to get it out of a situation it is stuck in. ROBOT_POSE This makes the tracking point and lookahead point always be at the same position as the drone's odometry. This is useful for before takeoff, when the robot may be carried around so that the location where the takeoff starts is at the drone's position."},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#subscriptions","title":"Subscriptions","text":"Topic Type Description <code>~/odometry</code> nav_msgs/Odometry Odometry of the robot. <code>~/trajectory_segment_to_add</code> airstack_msgs/TrajectoryXYZVYaw For ADD_SEGMENT mode, this is the trajectory segment to add to the current trajectory. <code>~/trajectory_override</code> airstack_msgs/TrajectoryXYZVYaw For TRACK mode, this overrides the current trajectory and makes the robot follow this directly."},{"location":"robot/ros_ws/src/autonomy/3_local/c_controls/trajectory_controller/#publications","title":"Publications","text":"Topic Type Description <code>~/tracking_point</code> TODO TODO <code>~/look_ahead</code> TODO TODO <code>~/traj_drone_point</code> TODO TODO <code>~/virtual_tracking_point</code> TODO TODO <code>~/closest_point</code> TODO TODO <code>~/trajectory_completion_percentage</code> TODO TODO <code>~/trajectory_time</code> TODO TODO <code>~/tracking_error</code> TODO TODO <code>~/velocity_pub</code> TODO TODO <code>~/debug_markers</code> TODO TODO <code>~/trajectory_vis</code> TODO TODO"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/","title":"VDB Mapping Core Library","text":"<p>DISCLAIMER: This library is still under development. Be warned that some interfaces will be changed and/or extended in the future.</p> <p>The VDB Mapping core library was primarily developed to be used in combination with the corresponding ROS wrapper or ROS2 wrapper</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/#getting-started","title":"Getting Started","text":""},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/#requirements","title":"Requirements","text":"<p>This library requires OpenVDB as it is build around it.  This library was initially developed using Version 5.0 and should work with all versions above.  </p> <p>As the apt packages are quite outdated for most systems, we recommend building at least OpenVDB v9.0.0 from source using the provided build instructions</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/#build-instructions","title":"Build instructions","text":"<p>The library can be either used as plain c++ library or in combination with the afore mentioned ROS wrapper.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/#dependencies","title":"Dependencies","text":"<p>The library requires the following dependencies to build correctly <pre><code>apt-get install -y libeigen3-dev\napt-get install -y libtbb-dev\napt-get install -y libpcl-dev\napt-get install -y libilmbase-dev\n</code></pre></p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/#plain-cmake","title":"Plain cmake","text":"<p>To build this package as a standalone library, follow the usual cmake building steps: <pre><code>git clone https://github.com/fzi-forschungszentrum-informatik/vdb_mapping\ncd vdb_mapping\nmkdir build &amp;&amp; cd build\ncmake ..\nmake -j8\nmake install\n</code></pre></p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/#ros-workspace","title":"ROS Workspace","text":"<p>In case you want build this library inside of a ROS workspace in combination with VDB Mapping ROS, you cannot use catkin_make since this library is not a catkin package. Instead you have to use catkin build or catkin_make_isolated to build the workspace.</p> <pre><code># source global ros\nsource /opt/ros/&lt;your_ros_version&gt;/setup.{zsh/bash}\n\n# create a catkin workspace\nmkdir -p ~/catkin_ws/src &amp;&amp; cd catkin_ws\n\n# clone packages\ngit clone https://github.com/fzi-forschungszentrum-informatik/vdb_mapping\n\n# install dependencies\nsudo apt update\nrosdep update\nrosdep install --from-paths src --ignore-src -y\n\n# build the workspace.\ncatkin build\n\n# source the workspace\nsource deve/setup.bash\n</code></pre>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/#ros2-workspace","title":"ROS2 Workspace","text":"<pre><code># source global ros\nsource /opt/ros/&lt;your_ros_version&gt;/setup.{zsh/bash}\n\n# create a catkin workspace\nmkdir -p ~/colcon_ws/src &amp;&amp; cd ~/colcon_ws/src\n\n# clone packages\ngit clone https://github.com/fzi-forschungszentrum-informatik/vdb_mapping\ngit clone https://github.com/fzi-forschungszentrum-informatik/vdb_mapping_ros2\n\n# install dependencies\nsudo apt update\nrosdep update\nrosdep install --from-paths src --ignore-src -y\n\n# build the workspace.  \ncolcon build\n\n# source the workspace\nsource install/setup.bash\n</code></pre>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/#citation","title":"Citation","text":"<p>Thanks that you read until here and please let us know if you run into any issues or have suggestions for improvements. If you use our work in your publications please feel free to cite our manuscript: <pre><code>  @inproceedings{besselmann2021vdb,\n  title={VDB-Mapping: a high resolution and real-time capable 3D mapping framework for versatile mobile robots},\n  author={Besselmann, M Grosse and Puck, Lennart and Steffen, Lea and Roennau, Arne and Dillmann, R{\\\"u}diger},\n  booktitle={2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},\n  pages={448--454},\n  year={2021},\n  organization={IEEE}\n  doi={10.1109/CASE49439.2021.9551430}}\n}\n</code></pre> or for the remote mapping case: <pre><code>@incollection{besselmann2022remote,\n  title={Remote VDB-Mapping: A Level-Based Data Reduction Framework for Distributed Mapping},\n  author={Besselmann, Marvin Grosse and R{\\\"o}nnau, Arne and Dillmann, R{\\\"u}diger},\n  booktitle={Robotics in Natural Settings: CLAWAR 2022},\n  pages={448--459},\n  year={2022},\n  publisher={Springer}\n  doi={10.1007/978-3-031-15226-9_42}\n}\n</code></pre></p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping/#acknowledgement","title":"Acknowledgement","text":"<p>The research leading to this package has received funding from the German Federal Ministry of Education and Research under grant agreement No. 13N14679:  </p> <p> </p> <p>ROBDEKON - Robotic systems for decontamination in hazardous environments More information: robdekon.de </p> <p> </p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/","title":"VDB Mapping ROS2 Package","text":"<p>DISCLAIMER: This library is still under development. Be warned that some interfaces will be changed and/or extended in the future.</p> <p>The VDB Mapping ROS2 Package is a ROS2 wrapper around VDB Mapping</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#getting-started","title":"Getting Started","text":""},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#requirements","title":"Requirements","text":"<p>This library requires OpenVDB as it is build around it. This library was initially developed using Version 5.0 and should work with all versions above. Either use the apt package which will be automatically installed via rosdep or compile the package from source using the provided build instructions</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#build-instructions","title":"Build instructions","text":"<p>Since the required VDB Mapping library is a plain C++ package, you cannot use catkin_make directly. Instead you have to use colcon build to build the workspace.</p> <pre><code># source global ros\nsource /opt/ros/&lt;your_ros_version&gt;/setup.bash\n\n# create a catkin workspace\nmkdir -p ~/colcon_ws/src &amp;&amp; cd ~/colcon_ws/src\n\n# clone packages\ngit clone https://github.com/fzi-forschungszentrum-informatik/vdb_mapping\ngit clone https://github.com/fzi-forschungszentrum-informatik/vdb_mapping_ros2\n\n# install dependencies\nsudo apt update\nrosdep update\nrosdep install --from-paths src --ignore-src -y\n\n# build the workspace.  \ncolcon build\n\n# source the workspace\nsource install/setup.bash\n</code></pre>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#remote-mapping","title":"Remote Mapping","text":"<p>The remote mapping module is at the moment mainly designed to provide an external access to the map, without saving and loading it in a different node. In our case we use it to visualize the map data on a remote base station which is then used to control the robots from far away and give the user an overview of the area the robot operates in. Basically it is a full additional mapping node which can access the data chunks generated by the main mapping on the robot. These can the be integrated into the remote mapping instance to generate the same or a similar map depending on the chosen parameter settings. Diving deeper into the remote mapping functionality you might stumble over the term of updates, overwrites and sections. These are different mechanics to update a remote mapping instance. The first two are just a byproduct of the normal mapping process. Therefore there is no additional computation needed to generate the data for a remote mapping instance. However all three mechanics use OpenVDB grids which are compressed and serialized as a bitstream before sending them out over the network.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#updates","title":"Updates","text":"<p>Updates are grids generated during the raycasting process and are also used for the internal map update. Here all sensor data is previously accumulated into an update grid which specifies whether the occupancy probability in the map should be updated as occupied or free. This two step process helps to prevent discretization issues due to the larger resolution of the grid compared to the raw sensor data. It is also possible to accumulate multiple sensor sources and measurements into one update grid (see accumulation parameters). The update grid can then be applied to the local or remote instance to generate the same map on both sides.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#overwrites","title":"Overwrites","text":"<p>Overwrite grids are similar to the update grid but only contain the voxel that have changed their occupancy state in the last iteration. Therefore they are more lightweight in terms of necessary bandwith. This can be useful in cases where the network is rather limited but has the downside, that the probabilistic framework is lost on the remote side.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#sections","title":"Sections","text":"<p>In contrast to updates and overwrites, sections are not bound to the sensor data rate but are generated with a defined rate and create additional processing load. Here the binary occupancy state of a section around a specified frame is copied out of the map and transformed into an update grid that can be applied to a remote instance. Although this created additional overhead it is still highly useful in certain scenarios. In our case we had to handle transmission delay and package loss where both the update and overwrites would result in incomplete maps since grids were lost. Sections on the other hand contain the complete chunk of the map while still being small with respect to the bandwidth.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#ros-api","title":"ROS API","text":""},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#ros-parameters","title":"ROS Parameters","text":"<p>VDB Mapping is highly configurable using ROS parameters. Below is a complete list of all available parameters. Internally we store them in a yaml configuration file. An example can be found here.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#basic-parameters","title":"Basic Parameters","text":"<p>Listed below are the general parameters to configure the basic behavior of vdb_mapping</p> Parameter Name Type Default Information map_frame string ' ' Coordinate frame of the map robot_frame string ' ' Coordinate frame of the robot max_range double 15.0 Global maximum raycasting range (can also be set for each sensor source individually) resolution double 0.07 Map resolution prob_hit double 0.7 Probability update if a beam hits a voxel prob_miss double 0.4 Probability update if a beam misses a voxel prob_thres_min double 0.12 Lower occupancy threshold of a voxel prob_thres_max double 0.97 Upper occupancy threshold of a voxel map_save_dir string ' ' Storage location for saved maps accumulate_updates boolean false Specifies whether the data of multiple sensor measurement should be accumulated before integrating it into the map. accumulation_period double 1 Specifies how long updates should be accumulated before integration visualization_rate double 1 Specifies in which rate the visualization is published publish_pointcloud boolean true Specifies whether the map should be published as pointcloud publish_vis_marker boolean true Specifies whether the map should be published as visual marker apply_raw_sensor_data boolean true Specify whether raw sensor data (e.g. Pointclouds) should be integrated into the map. This flag becomes relevant when the user wants to use a pure remote instance. If set to true all sensor sources will be periodically integrated into the map. sources string[] [] List of sensor sources remote_sources string[] [] List of remote sources"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#sensor-sources","title":"Sensor Sources","text":"<p>VDB Mapping is able to integrate an arbitrary amount of different sensor sources (given they are available as PointCloud2 msg). To add a new sensor source the user has to add it to the list of sources in the basic parameters. Below are listed the parameters that can be used to configure the individual sensor source. These have to be put into the corresponding namespace in the yaml file. For an example see this configuration file.</p> Parameter Name Type Default Information topic string ' ' Topic name of the sensor msg sensor_origin_frame string ' ' Sensor frame of the measurement. This parameter is optional and in general the frame id of the msg is used. However in some cases like already frame aligned point clouds this info is no longer the origin of the sensor. For this purpose the user can specify here from which frame the raycasting should be performed. max_range double 0 Individual per sensor max raycasting range. This parameter is optional and per default the global max range is used."},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#remote-sources","title":"Remote Sources","text":"<p>VDB Mapping provides a remote operation mode. Here the map data is shared between multiple instances to generate the same or similar maps (depending on the chosen parameter settings). For this remote sources can be specified over the parameter server.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#local-side","title":"Local side","text":"Parameter Name Type Default Information publish_updates boolean false Specifies whether updates are published publish_overwrites boolean false Specifies whether overwrites are published publish_sections boolean false Specifies whether sections are published section_update/rate double 1 Rate in which the section update is published section_update/frame string robot_frame Center of the update section section_update/min_coord/{x,y,z} double -10 Min coordinate of the section bounding box centered around the section frame section_update/max_coord/{x,y,z} double 10 Max coordinate of the section bounding box centered around the section frame"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#remote-side","title":"Remote side","text":"<p>Below are listed the parameters that can be used to configure the individual remote source. These have to be put into the corresponding namespace in the yaml file. An example config file can be found here.</p> Parameter Name Type Default Information namespace string ' ' Namespace of the remote mapping instance apply_remote_updates boolean false Specifies whether remote updates should be applied to the map apply_remote_overwrites boolean false Specifies whether remote overwrites should be applied to the map apply_remote_sections boolean false Specifies whether remote sections should be applied to the map"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#advertised-ros-topics","title":"Advertised ROS Topics","text":"Topic Name Type Information ~/vdb_map_visualization visualization_msgs/Marker Map visualization topic as voxel marker ~/vdb_map_pointcloud sensor_msgs/PointCloud2 Map visualization topic as pointcloud ~/vdb_map_updates vdb_mapping_msgs/UpdateGrid Map updates topic ~/vdb_map_overwrites vdb_mapping_msgs/UpdateGrid Map overwrites topic ~/vdb_map_sections vdb_mapping_msgs/UpdateGrid Map sections topic"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#subscribed-ros-topics","title":"Subscribed ROS Topics","text":"Topic Name Type Information {Parameter:sensor_source}/{Parameter:topic} sensor_msgs/PointCloud2 Pointcloud sensor message {Parameter:remote_source}/vdb_map_updates vdb_mapping_msgs/UpdateGrid Map updates topic {Parameter:remote_source}/vdb_map_overwrites vdb_mapping_msgs/UpdateGrid Map overwrites topic {Parameter:remote_source}/vdb_map_sections vdb_mapping_msgs/UpdateGrid Map sections topic"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#ros-services","title":"ROS Services","text":"Service Name Type Information ~/load_map vdb_mapping_msgs/LoadMap Loads the map specified in the msg ~/save_map std_srvs/Trigger Saves the current map in the destination specified in the map_save_dir parameter ~/reset_map std_srvs/Trigger Resets the current map ~/raytrace vdb_mapping_msgs/Raytrace Raytraces a point and returns coordinate where the ray first intersected the map ~/trigger_map_section_update vdb_mapping_msgs/TriggerMapSectionUpdate Triggers a map section update on a remote instance"},{"location":"robot/ros_ws/src/autonomy/4_global/a_world_models/vdb_mapping_ros2/#citation","title":"Citation","text":"<p>Thanks that you read until here and please let us know if you run into any issues or have suggestions for improvements. If you use our work in your publications please feel free to cite our manuscript: <pre><code>  @inproceedings{besselmann2021vdb,\n  title={VDB-Mapping: a high resolution and real-time capable 3D mapping framework for versatile mobile robots},\n  author={Besselmann, M Grosse and Puck, Lennart and Steffen, Lea and Roennau, Arne and Dillmann, R{\\\"u}diger},\n  booktitle={2021 IEEE 17th International Conference on Automation Science and Engineering (CASE)},\n  pages={448--454},\n  year={2021},\n  organization={IEEE}\n  doi={10.1109/CASE49439.2021.9551430}}\n}\n</code></pre> or for the remote mapping case: <pre><code>@incollection{besselmann2022remote,\n  title={Remote VDB-Mapping: A Level-Based Data Reduction Framework for Distributed Mapping},\n  author={Besselmann, Marvin Grosse and R{\\\"o}nnau, Arne and Dillmann, R{\\\"u}diger},\n  booktitle={Robotics in Natural Settings: CLAWAR 2022},\n  pages={448--459},\n  year={2022},\n  publisher={Springer}\n  doi={10.1007/978-3-031-15226-9_42}\n}\n</code></pre></p>"},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/","title":"Random Walk Global Planner Baseline","text":"<p>The Random Walk Planner serves as a baseline global planner for stress testing system autonomy. Unlike more informed and intelligent planners, the Random Walk Planner generates a series of random trajectories to evaluate system robustness. Using the map published by VDB, the planner will generate and publish multiple linked straight-line trajectories, checking for collisions along these paths.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#functionality","title":"Functionality","text":"<p>Upon activation by the behavior tree, the Random Walk Planner will:</p> <ol> <li>Generate a specified number of straight-line path segments.</li> <li>Continuously monitor the robot's progress along the published path.</li> <li>Once the robot completes the current path, a new set of paths will be generated.</li> </ol> <p>This loop continues, allowing the system to explore various trajectories and stress test the overall autonomy stack.</p>"},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#parameters","title":"Parameters","text":"Parameter Description <code>num_paths_to_generate</code> Number of straight-line paths to concatenate into a complete trajectory. <code>max_start_to_goal_dist_m</code> Maximum distance (in meters) from the start point to the goal point for each straight-line segment. <code>checking_point_cnt</code> Number of points along each straight-line segment to check for collisions. <code>max_z_change_m</code> Maximum allowed change in height (z-axis) between the start and goal points. <code>collision_padding_m</code> Extra padding (in meters) added around a voxel's dimensions when checking for collisions. <code>path_end_threshold_m</code> Distance threshold (in meters) for considering the current path completed and generating a new one. <code>max_z_angle_change_rad</code> Maximum allowed change in angle (in radians) between consecutive straight-line segments to ensure a relatively consistent direction. <code>robot_frame_id</code> The frame name for the robot's base frame to look up the transform from the robot position to the world."},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#services","title":"Services","text":"Parameter Type Description <code>~/global_plan_toggle</code> std_srvs/Trigger A toggle switch to turn on and off the random walk planner."},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#subscriptions","title":"Subscriptions","text":"Parameter Type Description <code>~/sub_map_topic</code> visualization_msgs/Marker Stores the map representation that is output from the world or local map topic; currently using vdb local map. <code>~/tf</code> geometry_msgs/TransformStamped Stores the transform from the robot to the world."},{"location":"robot/ros_ws/src/autonomy/4_global/b_planners/random_walk/#publications","title":"Publications","text":"Parameter Type Description <code>~/pub_global_plan_topic</code> nav_msgs/Path Outputs the global plan that is generated from the random walk planner."},{"location":"robot/ros_ws/src/autonomy/5_behavior/rqt_fixed_trajectory_generator/","title":"RQT Python FixedTrajectoryGenerator","text":"<p>If you <code>colcon build</code> this package in a workspace and then run \"rqt --force-discover\" after sourcing the workspace, the plugin should show up as \"Fixed Trajectory Generator\" in \"Miscellaneous Tools\" in the \"Plugins\" menu.</p> <p>You can use the <code>generate_rqt_py_package.sh</code> script to generate a new package by doing the following from the rqt_fixed_trajectory_generator directory</p> <pre><code>./generate_rqt_py_package.sh [package name] [class name] [plugin title]\n</code></pre> <p>[package name] will be the name of the package and a directory with this name will be created above <code>rqt_fixed_trajectory_generator/</code>. [class name] is the name of the class in <code>src/[package name]/template.py</code>. [plugin title] is what the plugin will be called in the \"Miscellaneous Tools\" menu.</p> <p>For example,</p> <pre><code>cd rqt_fixed_trajectory_generator/\n./generate_rqt_py_package.sh new_rqt_package ClassName \"Plugin Title\"\n</code></pre>"},{"location":"robot/ros_ws/src/autonomy/5_behavior/rqt_py_template/","title":"RQT Python Template","text":"<p>If you <code>colcon build</code> this package in a workspace and then run \"rqt --force-discover\" after sourcing the workspace, the plugin should show up as \"PluginTitle\" in \"Miscellaneous Tools\" in the \"Plugins\" menu.</p> <p>You can use the <code>generate_rqt_py_package.sh</code> script to generate a new package by doing the following from the rqt_py_template directory</p> <pre><code>./generate_rqt_py_package.sh [package name] [class name] [plugin title]\n</code></pre> <p>[package name] will be the name of the package and a directory with this name will be created above <code>rqt_py_template/</code>. [class name] is the name of the class in <code>src/[package name]/template.py</code>. [plugin title] is what the plugin will be called in the \"Miscellaneous Tools\" menu.</p> <p>For example,</p> <pre><code>cd rqt_py_template/\n./generate_rqt_py_package.sh new_rqt_package ClassName \"Plugin Title\"\n</code></pre>"},{"location":"simulation/gazebo/","title":"Index","text":"<p>In the future we could support Gazebo.</p>"},{"location":"simulation/isaac-sim/sitl_integration/docs/","title":"Usage","text":"<p>To enable this extension, run Isaac Sim with the flags --ext-folder {path_to_ext_folder} --enable {ext_directory_name}</p>"},{"location":"simulation/isaac-sim/sitl_integration/docs/CHANGELOG/","title":"Changelog","text":""},{"location":"simulation/isaac-sim/sitl_integration/docs/CHANGELOG/#010-2024-07-10","title":"[0.1.0] - 2024-07-10","text":""},{"location":"simulation/isaac-sim/sitl_integration/docs/CHANGELOG/#added","title":"Added","text":"<ul> <li>Initial version of TEST_EXTENSION_TITLE Extension</li> </ul>"},{"location":"simulation/isaac-sim/sitl_integration/drag_and_drop/","title":"Index","text":"<p>Download the AscentAeroSystemsSITLPackage.zip from Google Drive and unzip into this directory.</p>"}]}